{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64b6c94f-3e79-46b7-b116-7027966777f8",
   "metadata": {},
   "source": [
    "# Mamba-2 Language Model demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa052505-d91c-4e87-8daa-2b00ad8cc881",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a74ee1cb-b4b2-46a8-98a4-1dd845c1e5ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zxy/miniconda3/envs/paper-r/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from mamba2 import Mamba2LMHeadModel\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59ab109-2cbe-4f7a-b5ce-a58b8860f98c",
   "metadata": {},
   "source": [
    "Official pretrained models on [huggingface](https://huggingface.co/state-spaces):\n",
    "* `state-spaces/mamba2-130m`\n",
    "* `state-spaces/mamba2-370m`\n",
    "* `state-spaces/mamba2-780m`\n",
    "* `state-spaces/mamba2-1.3b`\n",
    "* `state-spaces/mamba2-2.7b`\n",
    "\n",
    "Choose a model depending on available system RAM (for CPU or system with unified memory) or VRAM.\n",
    "\n",
    "Note that these are base models without fine-tuning for downstream tasks such as chat or instruction following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6569ffd-993f-4d5b-9094-902801fe6c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zxy/miniconda3/envs/paper-r/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = Mamba2LMHeadModel.from_pretrained(\"state-spaces/mamba2-1.3b\", device=device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neox-20b\")\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb837263-8a1f-40bf-a9b1-fce72225a674",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = dict(\n",
    "    max_new_length=200,\n",
    "    temperature=1.0,\n",
    "    top_k=30,\n",
    "    top_p=1.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87006a5d-7992-4026-9b40-36cbc3ebf8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt: str, seed: int = 0, show_perf: bool = True):\n",
    "    \"\"\"Generate streaming completion\"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    input_ids = tokenizer(prompt, return_tensors='pt').input_ids.to(device)[0]\n",
    "    print(prompt, end=\"\")\n",
    "\n",
    "    start = time.process_time()\n",
    "    n_generated = 0\n",
    "    for i, (token_id, _hidden_state) in enumerate(model.generate(input_ids, **generation_config)):\n",
    "        token = tokenizer.decode([token_id])\n",
    "        if i == 0:\n",
    "            now = time.process_time()\n",
    "            prompt_eval_elapsed, start = now - start, now\n",
    "        else:\n",
    "            n_generated += 1\n",
    "        print(token, end=\"\", flush=True)\n",
    "    if show_perf:\n",
    "        elapsed = time.process_time() - start\n",
    "        print('\\n\\n---')\n",
    "        print(f'Prompt eval | tokens: {input_ids.shape[0]} | elapsed: {prompt_eval_elapsed:.2f}s | tok/s: {input_ids.shape[0] / prompt_eval_elapsed:.2f}')\n",
    "        print(f'Generation | tokens: {n_generated} | elapsed: {elapsed:.2f}s | tok/s: {n_generated / elapsed:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b926b16-2883-4eef-9459-3718498409e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mamba is a new state space model architecture, for applications such as neural signal processing and classification. It has been implemented as a C library for Windows.\n",
      "\n",
      "Features are:\n",
      "\n",
      "Vector autoregressive models of any order\n",
      "\n",
      "Bayesian state space models of any order\n",
      "\n",
      "SVM models of any order\n",
      "\n",
      "K-Means clustering models\n",
      "\n",
      "Support for linear transformations, including scaling, translation, rotation, and scaling+translation\n",
      "\n",
      "Support for random data as a distribution of samples for both the training and testing\n",
      "\n",
      "Support for non-linear transformation models by the choice of activation function through a weighted least squares cost\n",
      "\n",
      "The Mamba library can be downloaded at the following URL: https://github.com/louisdal/mamba\n",
      "\n",
      "---\n",
      "Prompt eval | tokens: 9 | elapsed: 1.34s | tok/s: 6.71\n",
      "Generation | tokens: 144 | elapsed: 4.25s | tok/s: 33.86\n"
     ]
    }
   ],
   "source": [
    "generate(\"Mamba is a new state space model architecture\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "608ccece-9a11-47bc-bafd-7b47fc6383c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The meaning of life is one that you choose. And this is what I want to tell you, as a person who has been through so much, and whose life will go on no matter what.\n",
      "\n",
      "---\n",
      "Prompt eval | tokens: 5 | elapsed: 0.21s | tok/s: 24.06\n",
      "Generation | tokens: 34 | elapsed: 1.02s | tok/s: 33.39\n"
     ]
    }
   ],
   "source": [
    "generate(\"The meaning of life is\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0bc8a2b-b2bc-4d30-bf4c-213baec7441a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is Nvidia's biggest moat, but you can build a strong case for it even without it. If you're making high-end gaming PC (Gigabytes of RAM, beefy graphics cards, beefy cooling systems).\n",
      "\n",
      "Nvidia's GPUs are the most powerful, reliable, and expensive parts in the industry. GPUs are very power hungry, so if they run hot, things can get complicated really fast (I learned this by the ways of my Razer Core. A lot!).\n",
      "\n",
      "If you're looking to build a gaming PC or something that needs lots of RAM, you can build a PC with a huge amount of RAM, but most people use them like me. Most of the times, you can get away with 8GB RAM.\n",
      "\n",
      "Then your graphics cards are your largest financial investment and your biggest power wasters. A good GPU can cost a few grand. But as Nvidia makes more and more powerful GPUs, the price comes down. It's hard to build a\n",
      "\n",
      "---\n",
      "Prompt eval | tokens: 9 | elapsed: 0.37s | tok/s: 24.32\n",
      "Generation | tokens: 199 | elapsed: 5.89s | tok/s: 33.80\n"
     ]
    }
   ],
   "source": [
    "generate(\"CUDA is Nvidia's biggest moat\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
